{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extraktionsbaserad textsammanfattare med olika rankningsmått "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports --> TODO fixa ordning \n",
    "# Web scrapping -->  module för att ladda ner artiklar \n",
    "from newspaper import Article\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "import nltk \n",
    "import ssl\n",
    "import re  # används ej någonstans (tror jag) / björn\n",
    "import regex as rex # \n",
    "\n",
    "\n",
    "# Summarization length of original text\n",
    "percentage = 0.15\n",
    "n_sentences = 3\n",
    "\n",
    "# Fixes some errors, found online at https://github.com/gunthercox/ChatterBot/issues/930#issuecomment-322111087\n",
    "try:\n",
    "    _create_unverified_https_context = ssl._create_unverified_context\n",
    "except AttributeError:\n",
    "    pass\n",
    "else:\n",
    "    ssl._create_default_https_context = _create_unverified_https_context"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lägg till nya artiklar \n",
    "\n",
    "* Hämta komplett url \n",
    "* Lägg till i python cellen under här\n",
    "* Tilldela till variabeln text \n",
    "* Tryck Restart och sen Run All \n",
    "* Filerna hamnar i mappen \"summarizations\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#text = \"https://www.aftonbladet.se/nyheter/a/kE6ExL/sd-far-tunga-poster-i-utskotten\"\n",
    "#text = 'https://www.svt.se/nyheter/utrikes/stall-dina-fragor-om-kriget-till-svt-s-utrikesreportrar'\n",
    "#text = \"https://www.aftonbladet.se/nyheter/a/kE6j0a/uppgifter-viktigt-fynd-i-jakten-pa-mordarna-i-sodertalje\"\n",
    "text = 'https://www.svt.se/nyheter/utrikes/foi-expert-varnar-for-allvarliga-luckor-i-undervattensbevakningen'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "article = Article(text, language='sv')\n",
    "article.download()\n",
    "article.parse()\n",
    "text = article.text\n",
    "homepage = article.meta_data['al']['ios']['app_name']\n",
    "link = article.url\n",
    "title = article.title\n",
    "\n",
    "# Beroende på vilken hemsida nyheten kommer ifrån kan titeln och texten inehålla delar av sidan man egentligen inte bryr sig om\n",
    "# T.ex. från aftonbladet är titeln med i texten och texten innehåller en mening som: \"publicerad: 30 sep\", man kan ta bort detta men \n",
    "# det blir om vi får tid över.\n",
    "\n",
    "#print('Title:' , article.title, '\\n\\nText: \\n', text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "### Calculate number of sentences to keep\n",
    "\n",
    "### List 1 - sentences\n",
    "* Varje mening separat\n",
    "\n",
    "### Dataframe - scores\n",
    "#### columns are the score of each ranking measure\n",
    "* Baseline\n",
    "* Headings\n",
    "* TF/IDF-score\n",
    "* NER\n",
    "* ~~Class~~ //Om vi har tid för ML\n",
    "\n",
    "### List 2 -> Cleaned for Stop Words \n",
    "* Varje mening separat \n",
    "\n",
    "###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Original Sentences List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['INSÄNDARE',\n",
       " 'Socialdemokraternas partiledare Magdalena Andersson kallar till presskonferens för att meddela ett historiskt beslut: ”Det Socialdemokratiska arbetarpartiet har nu kommit till vägs ände, vi har fullgjort vårt uppdrag med att göra rättvisa för och lyfta det arbetande folket',\n",
       " 'Vi låter nu andra politiska krafter ta vid, som är mer lämpade att lösa 2000-talets problem',\n",
       " 'Vi socialdemokrater avvecklar nu vårt politiska parti och tackar samtidigt för hundra år av förtroende.”',\n",
       " '']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# removes endlines:\n",
    "from token import NEWLINE\n",
    "\n",
    "org_sentences = text.replace('\\n\\n', '. ')\n",
    "# creates some exceptions from above rule\n",
    "org_sentences = org_sentences.replace('.. ', '. ')\n",
    "org_sentences = org_sentences.replace(':. ', ': ')\n",
    "org_sentences = org_sentences.split('. ')\n",
    "\n",
    "sentences_to_remove = []\n",
    "for i, sentence in enumerate(org_sentences): \n",
    "    if sentence == title: \n",
    "        sentences_to_remove.append(sentence)\n",
    "    if 'publicerad:' in sentence.lower(): \n",
    "        sentences_to_remove.append(sentence)\n",
    "\n",
    "org_sentences = [sentence for sentence in org_sentences if sentence not in sentences_to_remove]\n",
    "\n",
    "org_sentences[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 28 entries, 0 to 27\n",
      "Data columns (total 4 columns):\n",
      " #   Column    Non-Null Count  Dtype\n",
      "---  ------    --------------  -----\n",
      " 0   Baseline  28 non-null     int64\n",
      " 1   Headings  28 non-null     int64\n",
      " 2   TF        28 non-null     int64\n",
      " 3   NER       28 non-null     int64\n",
      "dtypes: int64(4)\n",
      "memory usage: 1.0 KB\n"
     ]
    }
   ],
   "source": [
    "index = range(org_sentences.__len__())\n",
    "columns = ['Baseline', 'Headings', 'TF', 'NER']\n",
    "scores = pd.DataFrame(index=index, columns=columns)\n",
    "scores.fillna(0, inplace=True)\n",
    "scores.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create spacy doc object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "# Credit to Explosion for sv_core_news_sm --> https://github.com/explosion \n",
    "# \"lemmatization accuracy 0.95\"\n",
    "# Create spacy nlp object \n",
    "nlp = spacy.load(\"sv_core_news_sm\") # nlp used by lemmatizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['INSÄNDARE ', 'Socialdemokraterna partiledare Magdalena Andersson kalla till presskonfer för att meddela en historisk beslut : ” en socialdemokratisk arbetarparti ha nu komma till väga ände , vi ha fullgöra vi uppdrag med att göra rättvisa för och lyfta en arbeta folk ', 'vi låta nu annan politisk kraft ta vid , som vara mycket lämpa att lösa 2000-tal problem ', 'vi socialdemokrat avveckl nu vi politisk parti och tacka samtidig för hundra år av förtroende . ” ', '', 'denna vara vad som böra hända för cirka 15 år sedan , men aldrig komma att hända ', 'anledning vara att Socialdemokrater inte länge vara en politisk parti , den vara en maktstruktur ', 'man ha systematisk under 100 år skapa organisation och en föreningsliv som finnas närvarande i varje liten vrå av samhälle , LO , Folkets hus & park , Verdandi , PRO , lista kunna göra meterlång och denna garantera en marinerande av folk i en självdestruktiv värdegrund som idag prägla Sverige ', 'för hundra år sedan vara S en led i en uppryckning och samhällsbyggaranda som prägla Europa under 1900-tal ', 'idag ha vänster sig livsluft i att definiera och plocka fram tilltänkt väljargrupp och övertal denna att de vara förtryckt och marginalisera och behöva vänster hjälp ', 'Finns den inte länge tillräcklig många individ som vara mottaglig för denna lockton importera man ', 'här vara en muslimsk diaspora från Mellanöstern och Afrika perfekt grupp ', 'denna vara sällan sekulära utan en flesa lyda blin en helig skrifterna i vilken arbete vara underordnad en religiös riterna och en religiös levnadsregl som skola utföra under dygn vaken timme ', 'den bli inte mycket inkomst och pengar till mat och husrum av denna men ” en otrogen ” betala ju ', 'vi kalla den socialbidrag , flerbarnstillägg , efterlevandeersättning och etableringsersättning ', 'Muslimerna kalla den ” Jizyah ” , en muslimsk skyddsskatt som icke troende skola betala enligt koran ', '', 'demokratisk socialism ha ju en liten akilleshäl att man måste få 51 % av röst för att få sitta vid makt ', 'Viktig således att se till att valboskap stanna i utanförskap och bidragsberoende ', 'Bli en företagare och entreprenör kunna en ändra uppfattning och rösta ” fel ” i kommande val ', '', 'en annan exempel på uttaland vi aldrig få höra kunna vara att jämställdhetsmyndigheten generaldirektör Lena ag proklamera att denna myndighet skola lägga ner , eftersom önskvärd jämställdhet ha uppnåtta ', 'man prata aldrig om att fler chef inom stat och kommun nu vara kvinna , ingen utbildningsväg eller yrke vara stängd för kvinna , man prata bara om att näringsliv inte ha tillräcklig många kvinna i styrels ', 'när nöja man sig ? vad göra man när 100 % av chef vara kvinna ? ha vi då uppnå 100 % jämställdhet ? denna exempel på två självspelande pianon måste ta för vad de vara ', 'företeels som inverka extrem destruktiv på vi svensk samhällssystem och vi svensk välfärdssta ', '', 'Mats Olsson ', 'företagare , Östhammar ']\n"
     ]
    }
   ],
   "source": [
    "# den --> det \n",
    "# noterar detta att det är en dålig lemmatiserare --> språkbanken stanza / lemmy / kth...  \n",
    "\n",
    "def lemmatizer(list_of_strings):\n",
    "    lemmatized_sentences = []\n",
    "    lemmatized_sentence = ''\n",
    "    for i in range(len(list_of_strings)): \n",
    "        sentence_to_lemmatize = nlp(list_of_strings[i])\n",
    "        for token in sentence_to_lemmatize:\n",
    "            lemma = token.lemma_\n",
    "            lemmatized_sentence += lemma + ' '  \n",
    "        \n",
    "        lemmatized_sentences.append(lemmatized_sentence)\n",
    "        lemmatized_sentence = '' \n",
    "\n",
    "    return lemmatized_sentences\n",
    "\n",
    "# Created Lemmatized DS\n",
    "lemmatized_org_sentences = lemmatizer(org_sentences)\n",
    "print(lemmatized_org_sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Proper Nouns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Jizyah', 'Socialdemokraternas', 'Park', 'LO', 'PRO', 'Olsson', 'Lena', 'Sverige', 'Verdandi', 'Östhammar', 'Mellanöstern', 'Europa', 'Afrika', 'Mats', 'Andersson'}\n"
     ]
    }
   ],
   "source": [
    "# För bättre täckning på NER \n",
    "def proper_nouns(list_of_strings):\n",
    "    proper_nouns = set()\n",
    "    for i in range(len(list_of_strings)): \n",
    "        sentence_to_pos = nlp(list_of_strings[i])\n",
    "        for token in sentence_to_pos: \n",
    "            token_str = token.text\n",
    "            if token.pos_ == \"PROPN\" and len(token_str) > 1:\n",
    "                proper_nouns.add(token_str.strip())\n",
    "    return proper_nouns\n",
    "\n",
    "print(proper_nouns(org_sentences))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Named Entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def named_entity_recognition(list_of_strings):\n",
    "    doc = nlp(' '.join(list_of_strings))\n",
    "    # Convert tuple[Span] to str\n",
    "    named_entities = doc.ents.__str__()\n",
    "    # Remove string parenthesis \n",
    "    named_entities = named_entities[1:len(named_entities) - 1]\n",
    "    # Create list of strings\n",
    "    named_entities = named_entities.split(',')\n",
    "    \n",
    "    named_entities_set = set()\n",
    "    for entity in named_entities: \n",
    "        named_entities_set.add(entity.strip())\n",
    "    return named_entities_set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stop Word Filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspired by https://www.geeksforgeeks.org/removing-stop-words-nltk-python/\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.probability import FreqDist\n",
    "\n",
    "def get_swe_stop_words(): \n",
    "    swe_stop_words = set(stopwords.words('swedish'))\n",
    "    # Not stop word cleaning \n",
    "    swe_stop_words.update([',', '\"', ':', '-', '–', '”'])\n",
    "    return swe_stop_words\n",
    "\n",
    "def stop_word_filtering(list_of_strings):\n",
    "    word_tokens = word_tokenize(' '.join(list_of_strings))\n",
    "    filtered_sentence_into_singletons = [w for w in word_tokens if not w.lower() in get_swe_stop_words()]\n",
    "    return filtered_sentence_into_singletons"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Frequency Distribution "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def frequency_distribution(list): \n",
    "    fdist = FreqDist(word.lower() for word in word_tokenize(' '.join(list)))\n",
    "    return fdist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_words = stop_word_filtering(lemmatized_org_sentences) \n",
    "fdist = frequency_distribution(filtered_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ranking Measures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline\n",
    "(1/N --> n=ordning mening kommer i dvs. första meningen får N=1 -> 1/1, andra meningen får N=2 -> 1/2, osv.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Baseline</th>\n",
       "      <th>Headings</th>\n",
       "      <th>TF</th>\n",
       "      <th>NER</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>28.000000</td>\n",
       "      <td>28.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>28.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.140256</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.198050</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.035714</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.047078</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.069048</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.129464</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Baseline  Headings    TF   NER\n",
       "count  28.000000      28.0  28.0  28.0\n",
       "mean    0.140256       0.0   0.0   0.0\n",
       "std     0.198050       0.0   0.0   0.0\n",
       "min     0.035714       0.0   0.0   0.0\n",
       "25%     0.047078       0.0   0.0   0.0\n",
       "50%     0.069048       0.0   0.0   0.0\n",
       "75%     0.129464       0.0   0.0   0.0\n",
       "max     1.000000       0.0   0.0   0.0"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ranking metric 1 --> Baseline\n",
    "for i, score in enumerate(scores['Baseline']) :\n",
    "    scores['Baseline'][i] = 1/((i+1))\n",
    "scores.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Headings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Baseline</th>\n",
       "      <th>Headings</th>\n",
       "      <th>TF</th>\n",
       "      <th>NER</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>28.000000</td>\n",
       "      <td>28.000000</td>\n",
       "      <td>28.0</td>\n",
       "      <td>28.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.140256</td>\n",
       "      <td>1.071429</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.198050</td>\n",
       "      <td>1.303232</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.035714</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.047078</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.069048</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.129464</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Baseline   Headings    TF   NER\n",
       "count  28.000000  28.000000  28.0  28.0\n",
       "mean    0.140256   1.071429   0.0   0.0\n",
       "std     0.198050   1.303232   0.0   0.0\n",
       "min     0.035714   0.000000   0.0   0.0\n",
       "25%     0.047078   0.000000   0.0   0.0\n",
       "50%     0.069048   1.000000   0.0   0.0\n",
       "75%     0.129464   2.000000   0.0   0.0\n",
       "max     1.000000   5.000000   0.0   0.0"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ranking metric 2 --> Headings\n",
    "# Sets all 'Headings' scores to 0, mostly for testing so i can run this multiple times, \n",
    "# but also to make sure nothing weird has happened earlier in the code.\n",
    "scores['Headings'] = 0\n",
    "for i, sentence in enumerate(org_sentences): # Lemmatizerad? \n",
    "    for word in article.title.split(' '):\n",
    "        if word in sentence:\n",
    "            scores.at[i, 'Headings'] += 1\n",
    "scores.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sverigedemokraterna få ordförandepost i riksdag justitie och utrikesutskott \n",
    "[' få ', 'ordförandepost ', 'riksdag ', 'utrikesutskott ']\n",
    "\n",
    "* bara ord i lemma \n",
    "* ord i lemma med efter letters\n",
    "* ord i lemma med före letters --> x\n",
    "* före + lemma + efter "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Baseline</th>\n",
       "      <th>Headings</th>\n",
       "      <th>TF</th>\n",
       "      <th>NER</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>28.000000</td>\n",
       "      <td>28.000000</td>\n",
       "      <td>28.0</td>\n",
       "      <td>28.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.140256</td>\n",
       "      <td>0.214286</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.198050</td>\n",
       "      <td>0.686221</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.035714</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.047078</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.069048</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.129464</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Baseline   Headings    TF   NER\n",
       "count  28.000000  28.000000  28.0  28.0\n",
       "mean    0.140256   0.214286   0.0   0.0\n",
       "std     0.198050   0.686221   0.0   0.0\n",
       "min     0.035714   0.000000   0.0   0.0\n",
       "25%     0.047078   0.000000   0.0   0.0\n",
       "50%     0.069048   0.000000   0.0   0.0\n",
       "75%     0.129464   0.000000   0.0   0.0\n",
       "max     1.000000   3.000000   0.0   0.0"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocessed_title = stop_word_filtering(lemmatizer(title.split(' ')))\n",
    "p = rex.compile(r\"\\b\\S*\\L<words>\\S*\\b | \\b\\L<words>\\b | \\b\\L<words>\\S*\\b\", re.IGNORECASE, words=preprocessed_title)\n",
    "p.findall\n",
    "\n",
    "for i, sentence in enumerate(lemmatized_org_sentences):    \n",
    "    #print(sentence)\n",
    "    matches = p.findall(sentence)\n",
    "    #print(matches)\n",
    "    scores.at[i, \"Headings\"] = len(matches)\n",
    "\n",
    "scores.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TERM FREQUENCY \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Borrowed https://www.askpython.com/python/examples/tf-idf-model-from-scratch (with adjustments)\n",
    "#Term Frequency\n",
    "def termfreq(sentence, word):\n",
    "    N = len(sentence)\n",
    "    occurance = len([token for token in sentence if token == word])\n",
    "    return occurance/N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Borrowed https://www.askpython.com/python/examples/tf-idf-model-from-scratch (with adjustments)\n",
    "#Inverse Document Frequency\n",
    "def inverse_doc_freq(word):\n",
    "    try:\n",
    "        word_occurance = fdist.get(word) + 1\n",
    "    except:\n",
    "        word_occurance = 1\n",
    "    return np.log(len(lemmatized_org_sentences)/word_occurance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Borrowed https://www.askpython.com/python/examples/tf-idf-model-from-scratch (with adjustments)\n",
    "# TF*IDF-score \n",
    "def tf_idf(sentence):\n",
    "    score = 0\n",
    "    #tf_idf_vec = np.zeros((len(filtered_words),))\n",
    "    for word in sentence:\n",
    "        tf = termfreq(sentence,word)\n",
    "        idf = inverse_doc_freq(word)\n",
    "         \n",
    "        score += tf*idf\n",
    "        #tf_idf_vec[index_dict[word]] = value \n",
    "    return score\n",
    "\n",
    "for i, sentence in enumerate(lemmatized_org_sentences):\n",
    "    scores.at[i, 'TF'] = tf_idf(sentence)\n",
    "\n",
    "#scores.describe()\n",
    "#scores['TF'].idxmax()\n",
    "#scores.loc[scores['TF'] == scores['TF'].median()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF*IDF-score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://forketyfork.medium.com/latex-math-formulas-a-cheat-sheet-21e5eca70aae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ranking metric 3 --> TF*IDF\n",
    "\n",
    "# Term Weights --> Calculate importance of single words in text/doc\n",
    "# Binary term weights --> document specific\n",
    "# TF*IDF term weights --> document-collection specific \n",
    "\n",
    "# Assign weights to each dimension (attr/word) of each sentence (record/example) \n",
    "\n",
    "# Term Frequency (TF-score) --> TFij == frequency of the jth term in in the ith doc \n",
    "\n",
    "# Inverse Document Frequency \n",
    "# idf-score of the jth term measures the uniqueness of the jth term in the collection of documents\n",
    "# IDFj = log(M / Nj)\n",
    "#\n",
    "# M = total num of docs in collection \n",
    "# Nj is the number of documents that contain the jth term\n",
    "\n",
    "# HIGH TF*IDF-score \n",
    "# Word frequent in document && Occur in few documents of the collection \n",
    "# LOW TF*IDF-score\n",
    "# Not present in document || present in all documents of the collection "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NER "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Jizyah', 'under 1900-talet', 'PRO', 'LO', 'Magdalena Andersson', 'Lena', 'Östhammar', 'Mellanöstern', 'Europa', 'Afrika', 'idag', 'Socialdemokraternas', 'Park', 'Olsson', 'För hundra år sedan', 'Mats Olsson', 'Sverige', 'Verdandi', 'Mats', 'Andersson'}\n"
     ]
    }
   ],
   "source": [
    "scores['NER'] = 0\n",
    "named_entities = named_entity_recognition(org_sentences)\n",
    "proper_nouns = proper_nouns(org_sentences)\n",
    "ner_unique = named_entities.union(proper_nouns)\n",
    "print(ner_unique)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Baseline</th>\n",
       "      <th>Headings</th>\n",
       "      <th>TF</th>\n",
       "      <th>NER</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>3.998645</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.500000</td>\n",
       "      <td>3</td>\n",
       "      <td>59.234508</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.333333</td>\n",
       "      <td>0</td>\n",
       "      <td>23.779505</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.250000</td>\n",
       "      <td>1</td>\n",
       "      <td>23.322706</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.200000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Baseline  Headings         TF  NER\n",
       "0  1.000000         0   3.998645    0\n",
       "1  0.500000         3  59.234508    2\n",
       "2  0.333333         0  23.779505    0\n",
       "3  0.250000         1  23.322706    0\n",
       "4  0.200000         0   0.000000    0"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p = rex.compile(r\"\\L<words>\", words=ner_unique)\n",
    "\n",
    "for i, sentence in enumerate(org_sentences):\n",
    "    matches = p.findall(sentence)\n",
    "    scores.at[i, \"NER\"] = len(matches)\n",
    "\n",
    "scores.describe()\n",
    "scores.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combination Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Standardize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Baseline</th>\n",
       "      <th>Headings</th>\n",
       "      <th>TF</th>\n",
       "      <th>NER</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6.631063</td>\n",
       "      <td>-0.794998</td>\n",
       "      <td>-0.351159</td>\n",
       "      <td>-0.470231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.774645</td>\n",
       "      <td>10.334979</td>\n",
       "      <td>0.559938</td>\n",
       "      <td>1.078765</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.489173</td>\n",
       "      <td>-0.794998</td>\n",
       "      <td>-0.024880</td>\n",
       "      <td>-0.470231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.846437</td>\n",
       "      <td>2.914994</td>\n",
       "      <td>-0.032415</td>\n",
       "      <td>-0.470231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.460795</td>\n",
       "      <td>-0.794998</td>\n",
       "      <td>-0.417115</td>\n",
       "      <td>-0.470231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.203700</td>\n",
       "      <td>-0.794998</td>\n",
       "      <td>-0.003078</td>\n",
       "      <td>-0.470231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.020061</td>\n",
       "      <td>6.624987</td>\n",
       "      <td>0.013622</td>\n",
       "      <td>-0.470231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>-0.117668</td>\n",
       "      <td>-0.794998</td>\n",
       "      <td>0.710758</td>\n",
       "      <td>4.176756</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>-0.224791</td>\n",
       "      <td>-0.794998</td>\n",
       "      <td>-0.016615</td>\n",
       "      <td>1.853263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>-0.310489</td>\n",
       "      <td>-0.794998</td>\n",
       "      <td>0.191060</td>\n",
       "      <td>-0.470231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>-0.380605</td>\n",
       "      <td>-0.794998</td>\n",
       "      <td>-0.017133</td>\n",
       "      <td>-0.470231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>-0.439036</td>\n",
       "      <td>-0.794998</td>\n",
       "      <td>-0.149321</td>\n",
       "      <td>1.078765</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>-0.488477</td>\n",
       "      <td>-0.794998</td>\n",
       "      <td>0.409958</td>\n",
       "      <td>-0.470231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>-0.530855</td>\n",
       "      <td>-0.794998</td>\n",
       "      <td>0.031755</td>\n",
       "      <td>-0.470231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>-0.567583</td>\n",
       "      <td>-0.794998</td>\n",
       "      <td>-0.057104</td>\n",
       "      <td>-0.470231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>-0.599720</td>\n",
       "      <td>-0.794998</td>\n",
       "      <td>-0.011317</td>\n",
       "      <td>0.304267</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>-0.628076</td>\n",
       "      <td>-0.794998</td>\n",
       "      <td>-0.417115</td>\n",
       "      <td>-0.470231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>-0.653281</td>\n",
       "      <td>-0.794998</td>\n",
       "      <td>0.060323</td>\n",
       "      <td>-0.470231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>-0.675834</td>\n",
       "      <td>-0.794998</td>\n",
       "      <td>-0.094166</td>\n",
       "      <td>-0.470231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>-0.696130</td>\n",
       "      <td>-0.794998</td>\n",
       "      <td>-0.009103</td>\n",
       "      <td>-0.470231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>-0.714494</td>\n",
       "      <td>-0.794998</td>\n",
       "      <td>-0.417115</td>\n",
       "      <td>-0.470231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>-0.731189</td>\n",
       "      <td>-0.794998</td>\n",
       "      <td>0.343037</td>\n",
       "      <td>0.304267</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>-0.746432</td>\n",
       "      <td>-0.794998</td>\n",
       "      <td>0.455200</td>\n",
       "      <td>-0.470231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>-0.760404</td>\n",
       "      <td>-0.794998</td>\n",
       "      <td>0.334731</td>\n",
       "      <td>-0.470231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>-0.773259</td>\n",
       "      <td>-0.794998</td>\n",
       "      <td>-0.050587</td>\n",
       "      <td>-0.470231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>-0.785125</td>\n",
       "      <td>-0.794998</td>\n",
       "      <td>-0.417115</td>\n",
       "      <td>-0.470231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>-0.796112</td>\n",
       "      <td>-0.794998</td>\n",
       "      <td>-0.334084</td>\n",
       "      <td>0.304267</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>-0.806314</td>\n",
       "      <td>-0.794998</td>\n",
       "      <td>-0.290957</td>\n",
       "      <td>0.304267</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Baseline   Headings        TF       NER\n",
       "0   6.631063  -0.794998 -0.351159 -0.470231\n",
       "1   2.774645  10.334979  0.559938  1.078765\n",
       "2   1.489173  -0.794998 -0.024880 -0.470231\n",
       "3   0.846437   2.914994 -0.032415 -0.470231\n",
       "4   0.460795  -0.794998 -0.417115 -0.470231\n",
       "5   0.203700  -0.794998 -0.003078 -0.470231\n",
       "6   0.020061   6.624987  0.013622 -0.470231\n",
       "7  -0.117668  -0.794998  0.710758  4.176756\n",
       "8  -0.224791  -0.794998 -0.016615  1.853263\n",
       "9  -0.310489  -0.794998  0.191060 -0.470231\n",
       "10 -0.380605  -0.794998 -0.017133 -0.470231\n",
       "11 -0.439036  -0.794998 -0.149321  1.078765\n",
       "12 -0.488477  -0.794998  0.409958 -0.470231\n",
       "13 -0.530855  -0.794998  0.031755 -0.470231\n",
       "14 -0.567583  -0.794998 -0.057104 -0.470231\n",
       "15 -0.599720  -0.794998 -0.011317  0.304267\n",
       "16 -0.628076  -0.794998 -0.417115 -0.470231\n",
       "17 -0.653281  -0.794998  0.060323 -0.470231\n",
       "18 -0.675834  -0.794998 -0.094166 -0.470231\n",
       "19 -0.696130  -0.794998 -0.009103 -0.470231\n",
       "20 -0.714494  -0.794998 -0.417115 -0.470231\n",
       "21 -0.731189  -0.794998  0.343037  0.304267\n",
       "22 -0.746432  -0.794998  0.455200 -0.470231\n",
       "23 -0.760404  -0.794998  0.334731 -0.470231\n",
       "24 -0.773259  -0.794998 -0.050587 -0.470231\n",
       "25 -0.785125  -0.794998 -0.417115 -0.470231\n",
       "26 -0.796112  -0.794998 -0.334084  0.304267\n",
       "27 -0.806314  -0.794998 -0.290957  0.304267"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Standardize\n",
    "scores_standardized = StandardScaler.fit_transform(self=StandardScaler(), X=scores)\n",
    "scores_standardized = pd.DataFrame(scores_standardized, columns=columns)\n",
    "scores_standardized['Baseline'] *= 1.5\n",
    "scores_standardized['Headings'] *= 2.5\n",
    "scores_standardized['TF'] *= 0.3\n",
    "scores_standardized\n",
    "\n",
    "# ÄNDRA COL NAME TF col --> TF*IDF TODO \n",
    "# COUNTER WEIGHT: \n",
    "# IF -->  \n",
    "# Baseline score low (e.g. below median)\n",
    "# && term_frequency high (note: not TF*IDF) (e.g. above upper quartile)\n",
    "# && NER high (e.g. above upper quartile)\n",
    "# THEN: \n",
    "# add_counterweight(sentence, counter_weight) \n",
    "# counter_weight:  \n",
    "# final rankposition = i -->  ∑ -1 (-1 may be too big weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculate Summarization Length (number of sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "summarization:  4 \n",
      "original:  28\n"
     ]
    }
   ],
   "source": [
    "num_of_org_sentences = len(org_sentences)\n",
    "summarization_num_sentences = round(num_of_org_sentences * percentage)\n",
    "\n",
    "print(\"summarization: \", summarization_num_sentences, \"\\noriginal: \", num_of_org_sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Combine\n",
    "* Combine the scores into one overall score\n",
    "* add weight and/or ML if time allows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 6 0 7]\n",
      "INSÄNDARE\n"
     ]
    }
   ],
   "source": [
    "# Combination Function\n",
    "# Här ligger ML om vi gör det  \n",
    "\n",
    "#final_score = scores_standardized.drop('TF', axis=1).sum(axis=1)\n",
    "final_score = scores_standardized.sum(axis=1)\n",
    "best_sentences = final_score.nlargest(summarization_num_sentences, keep='all').index.values\n",
    "print(best_sentences)\n",
    "\n",
    "print(org_sentences[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Similarity heurstic \n",
    "# Rubrik --> Hur mycket lika --> Ta bort redundans \n",
    "# 5 fem väldigt lika --> heuristik välja bort något av det     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assemble output \n",
    "* Reassemble according to overall score ranking\n",
    "* Output summarization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage:\n",
      "\n",
      "Socialdemokraterna – en maktstruktur, inte ett parti \n",
      "\n",
      "Socialdemokraternas partiledare Magdalena Andersson kallar till presskonferens för att meddela ett historiskt beslut: ”Det Socialdemokratiska arbetarpartiet har nu kommit till vägs ände, vi har fullgjort vårt uppdrag med att göra rättvisa för och lyfta det arbetande folket\n",
      "Anledningen är att Socialdemokraterna inte längre är ett politiskt parti, det är en maktstruktur\n",
      "INSÄNDARE\n",
      "Man har systematiskt under 100 år skapat organisationer och ett föreningsliv som finns närvarande i varje liten vrå av samhället, LO, Folkets Hus & Park, Verdandi, PRO, listan kan göras meterlång och dessa garanterar ett marinerande av folket i den självdestruktiva värdegrund som idag präglar Sverige\n",
      "\n",
      "\n",
      "\n",
      "N sentences:\n",
      "\n",
      "Socialdemokraternas partiledare Magdalena Andersson kallar till presskonferens för att meddela ett historiskt beslut: ”Det Socialdemokratiska arbetarpartiet har nu kommit till vägs ände, vi har fullgjort vårt uppdrag med att göra rättvisa för och lyfta det arbetande folket\n",
      "Anledningen är att Socialdemokraterna inte längre är ett politiskt parti, det är en maktstruktur\n",
      "INSÄNDARE\n"
     ]
    }
   ],
   "source": [
    "# Percentage length of original text --> summarization \n",
    "print(\"Percentage:\\n\")\n",
    "print(title, \"\\n\")\n",
    "for i in best_sentences: \n",
    "    print(org_sentences[i])\n",
    "\n",
    "print(\"\\n\\n\")\n",
    "\n",
    "# N sentences \n",
    "print(\"N sentences:\\n\")\n",
    "for i in best_sentences[0:n_sentences]: \n",
    "    print(org_sentences[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Newspaper3k: \n",
      " Anledningen är att Socialdemokraterna inte längre är ett politiskt parti, det är en maktstruktur.\n",
      "Dessa är sällan sekulära utan de flest lyder blint de heliga skrifterna i vilka arbete är underordnat de religiösa riterna och det religiösa levnadsregler som skall utföras under dygnets vakna timmar.\n",
      "Vi kallar det socialbidrag, flerbarnstillägg, efterlevandeersättning och etableringsersättning.\n",
      "Muslimerna kallar det ”Jizyah”, den muslimska skyddsskatten som icke troende skall betala enligt koranen.\n",
      "Man pratar aldrig om att fler chefer inom stat och kommun nu är kvinnor, inga utbildningsvägar eller yrken är stängda för kvinnor, man pratar bara om att näringslivet inte har tillräckligt många kvinnor i styrelserna.\n"
     ]
    }
   ],
   "source": [
    "# Newspaper Summarization\n",
    "article.nlp()\n",
    "print(\"\\nNewspaper3k: \\n\", article.summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "summarization = link + '\\n\\n' + 'Title: ' + '\\n' + title + '\\n\\n' + 'Summarization:' + '\\n'\n",
    "for sentence in best_sentences: \n",
    "    summarization += org_sentences[sentence] + '\\n'\n",
    "summarization += '\\n\\n' + 'Newspaper3k:' + '\\n' + 'Title: ' + '\\n' + title + \"\\n\\n\" + article.summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os \n",
    "# FILEPATH = f'./summarizations/{homepage}/'\n",
    "# filename = f\"{FILEPATH}{title}.txt\"\n",
    "# os.makedirs(os.path.dirname(filename), exist_ok=True)\n",
    "\n",
    "# with open(filename, 'w') as f:\n",
    "#      f.write(summarization)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f84c718007c4e73e911614ffefbbc4d70113307eb785d27a429d66e7eb72440b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
