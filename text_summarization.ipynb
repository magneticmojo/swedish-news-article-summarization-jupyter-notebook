{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extraktionsbaserad textsammanfattare med olika rankningsmått "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "import nltk \n",
    "import ssl\n",
    "import re \n",
    "\n",
    "# Summarization length of original text\n",
    "percentage = 0.15\n",
    "\n",
    "# Fixes some errors, found online at https://github.com/gunthercox/ChatterBot/issues/930#issuecomment-322111087\n",
    "try:\n",
    "    _create_unverified_https_context = ssl._create_unverified_context\n",
    "except AttributeError:\n",
    "    pass\n",
    "else:\n",
    "    ssl._create_default_https_context = _create_unverified_https_context"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Web scrapping -->  module för att ladda ner artiklar \n",
    "from newspaper import Article\n",
    "text = \"https://www.aftonbladet.se/nyheter/a/kE6ExL/sd-far-tunga-poster-i-utskotten\"#'https://www.svt.se/nyheter/utrikes/stall-dina-fragor-om-kriget-till-svt-s-utrikesreportrar'\n",
    "article = Article(text, language='sv')\n",
    "article.download()\n",
    "article.parse()\n",
    "text = article.text\n",
    "\n",
    "# Beroende på vilken hemsida nyheten kommer ifrån kan titeln och texten inehålla delar av sidan man egentligen inte bryr sig om\n",
    "# T.ex. från aftonbladet är titeln med i texten och texten innehåller en mening som: \"publicerad: 30 sep\", man kan ta bort detta men \n",
    "# det blir om vi får tid över.\n",
    "\n",
    "#print('Title:' , article.title, '\\n\\nText: \\n', text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "### Calculate number of sentences to keep\n",
    "\n",
    "### List 1 - sentences\n",
    "* Varje mening separat\n",
    "\n",
    "### Dataframe - scores\n",
    "#### columns are the score of each ranking measure\n",
    "* Baseline\n",
    "* Headings\n",
    "* TF/IDF-score\n",
    "* NER\n",
    "* ~~Class~~ //Om vi har tid för ML\n",
    "\n",
    "### List 2 -> Cleaned for Stop Words \n",
    "* Varje mening separat \n",
    "\n",
    "###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Original Sentences List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# removes endlines:\n",
    "from token import NEWLINE\n",
    "\n",
    "org_sentences = text.replace('\\n\\n', '. ')\n",
    "# creates some exceptions from above rule\n",
    "org_sentences = org_sentences.replace('.. ', '. ')\n",
    "org_sentences = org_sentences.replace(':. ', ': ')\n",
    "org_sentences = org_sentences.split('. ')\n",
    "\n",
    "org_sentences[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = range(org_sentences.__len__())\n",
    "columns = ['Baseline', 'Headings', 'TF', 'NER']\n",
    "scores = pd.DataFrame(index=index, columns=columns)\n",
    "scores.fillna(0, inplace=True)\n",
    "scores.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create spacy doc object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "# Credit to Explosion for sv_core_news_sm --> https://github.com/explosion \n",
    "# \"lemmatization accuracy 0.95\"\n",
    "# Create spacy nlp object \n",
    "nlp = spacy.load(\"sv_core_news_sm\") # nlp used by lemmatizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# den --> det \n",
    "# noterar detta att det är en dålig lemmatiserare --> språkbanken stanza / lemmy / kth...  \n",
    "\n",
    "def lemmatizer(list_of_strings):\n",
    "    lemmatized_sentences = []\n",
    "    lemmatized_sentence = ''\n",
    "    for i in range(len(list_of_strings)): \n",
    "        sentence_to_lemmatize = nlp(list_of_strings[i])\n",
    "        for token in sentence_to_lemmatize:\n",
    "            lemma = token.lemma_\n",
    "            lemmatized_sentence += lemma + ' '  \n",
    "        \n",
    "        lemmatized_sentences.append(lemmatized_sentence)\n",
    "        lemmatized_sentence = '' \n",
    "\n",
    "    return lemmatized_sentences\n",
    "\n",
    "# Created Lemmatized DS\n",
    "lemmatized_org_sentences = lemmatizer(org_sentences)\n",
    "print(lemmatized_org_sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Proper Nouns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# För bättre täckning på NER \n",
    "def proper_nouns(list_of_strings):\n",
    "    proper_nouns = set()\n",
    "    for i in range(len(list_of_strings)): \n",
    "        sentence_to_pos = nlp(list_of_strings[i])\n",
    "        for token in sentence_to_pos: \n",
    "            token_str = token.text\n",
    "            if token.pos_ == \"PROPN\" and len(token_str) > 1:\n",
    "                proper_nouns.add(token_str.strip())\n",
    "    return proper_nouns\n",
    "\n",
    "print(proper_nouns(org_sentences))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Named Entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def named_entity_recognition(list_of_strings):\n",
    "    doc = nlp(' '.join(list_of_strings))\n",
    "    # Convert tuple[Span] to str\n",
    "    named_entities = doc.ents.__str__()\n",
    "    # Remove string parenthesis \n",
    "    named_entities = named_entities[1:len(named_entities) - 1]\n",
    "    # Create list of strings\n",
    "    named_entities = named_entities.split(',')\n",
    "    \n",
    "    named_entities_set = set()\n",
    "    for entity in named_entities: \n",
    "        named_entities_set.add(entity.strip())\n",
    "    return named_entities_set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stop Word Filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspired by https://www.geeksforgeeks.org/removing-stop-words-nltk-python/\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.probability import FreqDist\n",
    "\n",
    "def get_swe_stop_words(): \n",
    "    swe_stop_words = set(stopwords.words('swedish'))\n",
    "    # Not stop word cleaning \n",
    "    swe_stop_words.update([',', '\"', ':', '-', '–', '”'])\n",
    "    return swe_stop_words\n",
    "\n",
    "def stop_word_filtering(list_of_strings):\n",
    "    word_tokens = word_tokenize(' '.join(list_of_strings))\n",
    "    filtered_sentence_into_singletons = [w for w in word_tokens if not w.lower() in get_swe_stop_words()]\n",
    "    return filtered_sentence_into_singletons"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Frequency Distribution "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def frequency_distribution(list): \n",
    "    fdist = FreqDist(word.lower() for word in word_tokenize(' '.join(list)))\n",
    "    return fdist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_words = stop_word_filtering(lemmatized_org_sentences) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ranking Measures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline\n",
    "(1/N --> n=ordning mening kommer i dvs. första meningen får N=1 -> 1/1, andra meningen får N=2 -> 1/2, osv.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ranking metric 1 --> Baseline\n",
    "for i, score in enumerate(scores['Baseline']) :\n",
    "    scores['Baseline'][i] = 1/((i+1))\n",
    "scores.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Headings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ranking metric 2 --> Headings\n",
    "# Sets all 'Headings' scores to 0, mostly for testing so i can run this multiple times, \n",
    "# but also to make sure nothing weird has happened earlier in the code.\n",
    "scores['Headings'] = 0\n",
    "for i, sentence in enumerate(org_sentences):\n",
    "    for word in article.title.split(' '):\n",
    "        if word in sentence:\n",
    "            scores.at[i, 'Headings'] += 1\n",
    "scores.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TERM FREQUENCY \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores['TF'] = 0\n",
    "\n",
    "fdist = frequency_distribution(filtered_words)\n",
    "\n",
    "for i, sentence in enumerate(lemmatized_org_sentences):\n",
    "    for word in sentence.split(' '):\n",
    "        #word = word.lemma\n",
    "        if word in fdist.keys():\n",
    "            scores.at[i, \"TF\"] += fdist.get(word)\n",
    "\n",
    "#scores.describe()\n",
    "#scores['TF'].idxmax()\n",
    "#scores.head\n",
    "#print(org_sentences[32])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF*IDF-score\n",
    "* Diskutera hur vi kan använda måtten \n",
    "* If similarity is close --> Similar content --> Remove redundance?  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://forketyfork.medium.com/latex-math-formulas-a-cheat-sheet-21e5eca70aae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ranking metric 3 --> TF*IDF\n",
    "\n",
    "# Term Weights --> Calculate importance of single words in text/doc\n",
    "# Binary term weights --> document specific\n",
    "# TF*IDF term weights --> document-collection specific \n",
    "\n",
    "# Assign weights to each dimension (attr/word) of each sentence (record/example) \n",
    "\n",
    "# Term Frequency (TF-score) --> TFij == frequency of the jth term in in the ith doc \n",
    "\n",
    "# Inverse Document Frequency \n",
    "# idf-score of the jth term measures the uniqueness of the jth term in the collection of documents\n",
    "# IDFj = log(M / Nj)\n",
    "#\n",
    "# M = total num of docs in collection \n",
    "# Nj is the number of documents that contain the jth term\n",
    "\n",
    "# HIGH TF*IDF-score \n",
    "# Word frequent in document && Occur in few documents of the collection \n",
    "# LOW TF*IDF-score\n",
    "# Not present in document || present in all documents of the collection "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NER \n",
    "* (nltk lib) --> (Meningar med Named Entities är troligtvis viktigare)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores['NER'] = 0\n",
    "named_entities = named_entity_recognition(org_sentences)\n",
    "proper_nouns = proper_nouns(org_sentences)\n",
    "ner_unique = named_entities.union(proper_nouns)\n",
    "print(ner_unique)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Borrowed from https://stackoverflow.com/questions/33406313/how-to-match-any-string-from-a-list-of-strings-in-regular-expressions-in-python\n",
    "#p = re.compile(r\"\\L<words>\", words=['fun', 'dum', 'sun', 'gum'])\n",
    "\n",
    "print(named_entities)\n",
    "for i, sentence in enumerate(org_sentences):\n",
    "    print(sentence)\n",
    "\n",
    "    matches = re.findall(r\"(?=(\\b\" + '|'.join(ner_unique) + r\"\\b))\", sentence) \n",
    "    print(matches)\n",
    "    scores.at[i, \"NER\"] = len(matches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "import regex as rex # $ pip install regex\n",
    "\n",
    "p = rex.compile(r\"\\L<words>\", words=ner_unique)\n",
    "\n",
    "for i, sentence in enumerate(org_sentences):\n",
    "    matches = p.findall(sentence)\n",
    "    scores.at[i, \"NER\"] = len(matches)\n",
    "\n",
    "scores.describe()\n",
    "scores.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combination Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Standardize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize\n",
    "scores_standardized = StandardScaler.fit_transform(self=StandardScaler(), X=scores)\n",
    "scores_standardized = pd.DataFrame(scores_standardized, columns=columns)\n",
    "scores_standardized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculate Summarization Length (number of sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_of_org_sentences = len(org_sentences)\n",
    "summarization_num_sentences = round(num_of_org_sentences * percentage)\n",
    "\n",
    "print(\"summarization: \", summarization_num_sentences, \"\\noriginal: \", num_of_org_sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Combine\n",
    "* Combine the scores into one overall score\n",
    "* add weight and/or ML if time allows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combination Function\n",
    "# Här ligger ML om vi gör det  \n",
    "\n",
    "final_score = scores_standardized.sum(axis=1)\n",
    "best_sentences = final_score.nlargest(summarization_num_sentences, keep='all').index.values\n",
    "print(best_sentences)\n",
    "\n",
    "print(org_sentences[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assemble output \n",
    "* Reassemble according to overall score ranking\n",
    "* Output summarization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assemble Output \n",
    "print(\"Percentage:\\n\")\n",
    "for i in best_sentences: \n",
    "    print(org_sentences[i])\n",
    "\n",
    "print(\"\\n\\n\")\n",
    "\n",
    "print(\"N sentences:\\n\")\n",
    "for i in best_sentences[0:3]: \n",
    "    print(org_sentences[i])\n",
    "\n",
    "# KANSKE GÖRA FIL ELLER DYLIKT TODO "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Newspaper Summarization\n",
    "article.nlp()\n",
    "print(\"\\nNewspaper3k: \\n\", article.summary)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f84c718007c4e73e911614ffefbbc4d70113307eb785d27a429d66e7eb72440b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
